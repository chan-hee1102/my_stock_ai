import streamlit as st
import pandas as pd
import os
import google.generativeai as genai

# 1. í˜ì´ì§€ ì´ˆê¸° ì„¤ì •
st.set_page_config(page_title="AI STOCK COMMANDER", layout="wide")

# 2. [ë¸”ë¡œê·¸ í•™ìŠµ] Gemini AI ì„¤ì • (models/ ê²½ë¡œ ëª…ì‹œ)
if "GEMINI_API_KEY" in st.secrets:
    try:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        # ë¸”ë¡œê·¸ í•´ê²°ì±…ëŒ€ë¡œ ëª¨ë¸ëª…ì„ ì •í™•íˆ ì§€ì •í•©ë‹ˆë‹¤.
        model = genai.GenerativeModel('models/gemini-1.5-flash')
    except Exception as e:
        st.error(f"AI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        model = None
else:
    st.error("Secretsì— GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    model = None

# 3. ë°ì´í„° ë¡œë“œ ë¡œì§
def load_data():
    out_dir = "outputs"
    if not os.path.exists(out_dir): return None
    files = [f for f in os.listdir(out_dir) if f.startswith("final_result_") and f.endswith(".csv")]
    if not files: return None
    latest_file = sorted(files)[-1]
    df = pd.read_csv(os.path.join(out_dir, latest_file))
    df['ì¢…ëª©ì½”ë“œ'] = df['ì¢…ëª©ì½”ë“œ'].astype(str).str.zfill(6)
    return df

data = load_data()

# --- ì„¸ì…˜ ìƒíƒœ ìœ ì§€ (ë‹µë³€ ì‚¬ë¼ì§ ë°©ì§€ í•µì‹¬) ---
if "messages" not in st.session_state:
    st.session_state.messages = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ
if data is not None and "selected_stock" not in st.session_state:
    st.session_state.selected_stock = data.iloc[0].to_dict()

# 4. í™”ë©´ ë ˆì´ì•„ì›ƒ êµ¬ì„±
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ğŸ“‚ í¬ì°© ì¢…ëª©")
    if data is not None:
        # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ êµ¬í˜„ (i+1)
        for i, (idx, row) in enumerate(data.iterrows()):
            if st.button(f"{i+1}. {row['ì¢…ëª©ëª…']} | {row['ê±°ë˜ëŒ€ê¸ˆ(ì–µ)']}ì–µ", key=f"btn_{i}"):
                st.session_state.selected_stock = row.to_dict()
                st.rerun()

with col2:
    stock = st.session_state.selected_stock
    st.title(f"ğŸ“Š {stock['ì¢…ëª©ëª…']} ë¶„ì„")
    st.info(f"ì¢…ëª©ì½”ë“œ: {stock['ì¢…ëª©ì½”ë“œ']} | ê±°ë˜ëŒ€ê¸ˆ: {stock['ê±°ë˜ëŒ€ê¸ˆ(ì–µ)']}ì–µ")
    
    st.divider()
    
    # --- AI ì±„íŒ… ì˜ì—­ ---
    st.subheader("ğŸ’¬ AI Commander ìƒë‹´")

    # [ì¤‘ìš”] ì €ì¥ëœ ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ ë¨¼ì € í™”ë©´ì— ê·¸ë¦½ë‹ˆë‹¤ (ì´ê²Œ ìˆì–´ì•¼ ì•ˆ ì‚¬ë¼ì§)
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ì¢…ëª©ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ ê¸°ë¡ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. AI ë‹µë³€ ìƒì„± ë° ê¸°ë¡
        if model:
            try:
                with st.chat_message("assistant"):
                    with st.spinner("ë¶„ì„ ì¤‘..."):
                        # ë¸”ë¡œê·¸ ì§€ì¹¨ì— ë”°ë¼ ë¬¸ë§¥ê³¼ í•¨ê»˜ ì§ˆë¬¸ ì „ë‹¬
                        context = f"ë‹¹ì‹ ì€ ì£¼ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. {stock['ì¢…ëª©ëª…']}ì— ëŒ€í•´ ë‹µí•˜ì„¸ìš”."
                        full_query = f"{context}\n\nì§ˆë¬¸: {prompt}"
                        
                        response = model.generate_content(full_query)
                        answer = response.text
                        
                        st.markdown(answer)
                        # ë‹µë³€ì„ ì„¸ì…˜ì— ì €ì¥ (ë‹¤ìŒ ë¦¬ëŸ° ë•Œ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ í•¨)
                        st.session_state.messages.append({"role": "assistant", "content": answer})
            except Exception as e:
                st.error(f"AI ì‘ë‹µ ì¤‘ ì—ëŸ¬: {e}")
        
        # ë§ˆì§€ë§‰ì— ë¦¬ëŸ°ì„ í˜¸ì¶œí•˜ì—¬ ìƒíƒœë¥¼ í™•ì •ì‹œí‚µë‹ˆë‹¤.
        st.rerun()